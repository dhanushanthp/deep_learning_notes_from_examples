{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "\n",
    "# Convert all the text as integer\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    # Get the total number of characters per batch\n",
    "    chars_per_batch = batch_size * n_steps\n",
    "\n",
    "    # Total Number of batchs\n",
    "    n_batches = len(arr) // chars_per_batch\n",
    "\n",
    "    # Get the full pack of charactors for each batch\n",
    "    arr = arr[:n_batches * chars_per_batch]\n",
    "\n",
    "    # Reshape the array by batch_size, Horizonal reshaping based on batch size.\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    # Create steps from mini batch, Keep in mind this is a matrix\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n + n_steps]\n",
    "\n",
    "        # At last batch the y will be sort by 1 charactor. That will create error in `tf.nn.dynamic_rnn` for array size mis match\n",
    "        y_temp = arr[:, n + 1:n + n_steps + 1]\n",
    "        \n",
    "        # Create a zero array and append with y output\n",
    "        y = np.zeros(x.shape, dtype=np.int32)\n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "        \n",
    "        # To handle memory\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31 64 57 72 76]\n",
      " [81 11  3  1 57]\n",
      " [57 63 70 65 62]\n",
      " [ 1 57  1 72 74]\n",
      " [61  1 76 64 57]\n",
      " [ 1 75 57 79  1]\n",
      " [59 76 61 60  1]\n",
      " [57 75  1 75 77]\n",
      " [61 60  1 65 76]\n",
      " [59 61  1 71 62]]\n",
      "[[64 57 72 76 61]\n",
      " [11  3  1 57 70]\n",
      " [63 70 65 62 65]\n",
      " [57  1 72 74 61]\n",
      " [ 1 76 64 57 70]\n",
      " [75 57 79  1 64]\n",
      " [76 61 60  1 79]\n",
      " [75  1 75 77 62]\n",
      " [60  1 65 76 75]\n",
      " [61  1 71 62  1]]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "batch_size = 10; n_steps=5\n",
    "batchs = get_batches(encoded, batch_size, n_steps)\n",
    "x, y = next(batchs)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tensorflow Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, n_steps):\n",
    "    inputs = tf.placeholder(\n",
    "        shape=[batch_size, n_steps], dtype=tf.int32, name='inputs')\n",
    "    \n",
    "    outputs = tf.placeholder(\n",
    "        shape=[batch_size, n_steps], dtype=tf.int32, name='outputs')\n",
    "    \n",
    "    keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "    \n",
    "    print('Shape of the input tensor: {}'.format(inputs.get_shape()))\n",
    "    print('Shape of the output tensor: {}'.format(outputs.get_shape()))\n",
    "    \n",
    "    return inputs, outputs, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "\n",
    "    # Single cell creation\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "\n",
    "    # Initialize different cells by calling single cell creation function\n",
    "    cells = tf.contrib.rnn.MultiRNNCell(\n",
    "        [build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    print('ALL LSTM cells creation: {}'.format(cells))\n",
    "\n",
    "    # Values that pass to another cell horizontally for memory or time-steps\n",
    "    initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cells, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, lstm_size, number_of_classes):\n",
    "    # lstm_output comes as list. Concat to create as array\n",
    "    pre_output = tf.concat(lstm_output, axis=1)\n",
    "\n",
    "    # Reshape the output matrix as [batch_size*n_steps X lstm_size]\n",
    "    # Each row output for each charactor. The number of rows will batch_size*n_steps.\n",
    "    output = tf.reshape(pre_output, [-1, lstm_size])\n",
    "\n",
    "    # Apply softmax function by creation a softmax layer\n",
    "    # `variable_scope` helps to change the default name for weight initialization.\n",
    "    #  Because RNN cell also have weights and bias with the default\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(\n",
    "            tf.truncated_normal((lstm_size, number_of_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(number_of_classes))\n",
    "\n",
    "    # Input for softmax function\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "    predictions = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return predictions, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_of_classes):\n",
    "\n",
    "    # convert the numbers to one hot encoding\n",
    "    target_one_hot = tf.one_hot(targets, num_of_classes)\n",
    "\n",
    "    # Change the shape similer to output from RNN\n",
    "    target = tf.reshape(target_one_hot, logits.get_shape())\n",
    "\n",
    "    # Apply cross entrophy\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=target)\n",
    "    \n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Optimizer or Simple gradient decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* `tf.trainable_variables`: weights and bias used during the training, This is assigned by TF\n",
    "\n",
    "* `tf.gradients`: Constructs symbolic partial derivatives of sum of ys w.r.t. x in xs. That return $\\delta w, \\delta b = tf.gradients(cost, [W, b])$. The `tf.gradients()` returns the gradient of cost wrt each tensor in the second argument as a list in the same order.\n",
    "\n",
    "* `tf.clip_by_global_norm` is a function to put the upper bound on the gradient value wrt to each variables. `grad_clip` is the upper bound.\n",
    "\n",
    "* AdamOptimizer is used to minimise the prediction error. It is variation of gradient descent. \n",
    "Here's a list on optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    # weights and bias used during the training, This is assigned by TF\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Tune the weights\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Building Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create the class of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps = 50, lstm_size=128, num_layers = 2, learning_rate=0.001,\n",
    "                grad_clip=5, sampling=False):\n",
    "\n",
    "        if sampling==True:\n",
    "            batch_size, num_steps = 1,1\n",
    "\n",
    "        # Reset all variabled that has mapped with this model\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Tensor input\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "        \n",
    "        # Tensor cells and state.\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        # Encode the inputs.\n",
    "        input_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Create a RNN network with LSTM cell, input and initial state\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, input_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        \n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training parameters\n",
    "* `batch_size`    - Number of sequences running through the network in one pass.\n",
    "* `num_steps`     - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size`     - The number of units in the hidden layers.\n",
    "* `num_layers`    - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob`     - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "## Tips and Tricks\n",
    "\n",
    "### Monitoring Validation Loss vs. Training Loss\n",
    "The most important quantity to keep track of is the difference between your **training loss** (printed during training) and the **validation loss** (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "- If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to **decrease your network size, or to increase dropout**. For example you could try dropout of 0.5 and so on.\n",
    "- If your training/validation loss are about equal then your model is **underfitting**. **Increase the size of your model (either number of layers or the raw number of neurons per layer)**\n",
    "\n",
    "### Approximate number of parameters\n",
    "\n",
    "The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "- The number of parameters in your model. This is printed when you start training.\n",
    "- The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    "These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "- I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "- I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "### Best models strategy\n",
    "\n",
    "The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). **Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.**\n",
    "\n",
    "It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end **take whatever checkpoint gave the best validation performance**.\n",
    "\n",
    "By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: (100, 100)\n",
      "Shape of the output tensor: (100, 100)\n",
      "ALL LSTM cells creation: <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x11e184a20>\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "# Print losses every N interations\n",
    "print_every_n = 50\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "# Store the model for best validation performance, Where the validation loss is low.\n",
    "saver = tf.train.Saver(max_to_keep=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1663...  0.3197 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.0855...  0.3228 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.7331...  0.3211 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4193...  0.3217 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.3242...  0.3211 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2082...  0.3230 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.1719...  0.3245 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.0444...  0.3230 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 1.9644...  0.3228 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9265...  0.3210 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 1.8887...  0.3217 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.7832...  0.3210 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.7954...  0.3223 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.7485...  0.3243 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.7314...  0.3212 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.6814...  0.3226 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.6486...  0.3229 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.6204...  0.3205 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.6005...  0.3231 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.5867...  0.3219 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.5922...  0.3202 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.5521...  0.3220 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.5449...  0.3222 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.4985...  0.3208 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.5504...  0.3227 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.4652...  0.3206 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.4674...  0.3230 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.4758...  0.3207 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.4351...  0.3215 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.3947...  0.3228 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.3883...  0.3233 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.3582...  0.3206 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.3970...  0.3216 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.3235...  0.3201 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.3480...  0.3239 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.3801...  0.3228 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.3283...  0.3219 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.3374...  0.3206 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.3994...  0.3218 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.3427...  0.3220 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.2991...  0.3215 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.2897...  0.3219 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.3059...  0.3224 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.3033...  0.3214 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.3135...  0.3224 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.2382...  0.3216 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.2618...  0.3214 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.2886...  0.3216 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.2452...  0.3216 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.2566...  0.3217 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.2599...  0.3206 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.2215...  0.3221 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.2577...  0.3212 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.1976...  0.3222 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.2183...  0.3234 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.2550...  0.3226 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.2263...  0.3210 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.2363...  0.3223 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.2670...  0.3214 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.2272...  0.3216 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.2127...  0.3219 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.1704...  0.3221 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.1789...  0.3215 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.1851...  0.3214 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.2122...  0.3208 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.1905...  0.3215 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.2037...  0.3211 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.1934...  0.3216 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.1866...  0.3218 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.1835...  0.3214 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.1746...  0.3223 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.1780...  0.3227 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.1825...  0.3220 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.1829...  0.3215 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.1389...  0.3215 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.1454...  0.3214 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.1509...  0.3205 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.1830...  0.3211 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.1512...  0.3220 sec/batch\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initialize all TF variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Tuple of initial state with 0 tensors\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            \n",
    "            # we pass the each function element of list. That return tuples. So we pick from each.\n",
    "            # batch_loss: Previos iteration loss\n",
    "            # new_state: previous iteration final_state will be the new_state for next iteration.\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "\n",
    "    # i{iteration number}_l{# hidden layer units}.ckpt\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This helps to pick the latest check point.\n",
    "tf.train.get_checkpoint_state('checkpoints')\n",
    "\n",
    "# Pick the model by it's name\n",
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "# Load the model\n",
    "saver.restore(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Predictions\n",
    "Once the model is trained. We are going to generate text from trained models. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`pick_top_n` function will pick the predictions from RNN model with each class probability. Each output is each charactor.\n",
    "\n",
    "\n",
    "Step 1\n",
    "* `np.squeeze`: Used to remove one-dimensional entry from the shape of the given array. Where the `preds` comes as *2-dimension array* with the shape of (1,83). But once we push through the squeeze function the preds become *1-dimensional array* or vector. This make sure that you get a single dimensional array.\n",
    "\n",
    "\n",
    "Step 2\n",
    "* `np.argsort` :Sort the array by probability of each value in the array. And return the index of the value based on sort order. *Ex:* X = [3, 2, 9, 0], Then the np.argsort(X) will be [3, 1, 0, 3], Where the position of the values has been given in sort order.\n",
    "* `p[np.argsort(p)[:-top_n]] = 0` : Pick the position of the values which are not in `top_n` and mark them as 0. Get the token index with the high probability.\n",
    "* `p / np.sum(p)`: Since this prob, But it's not sum to one, The following line help to make the sum as one, so that we have normalized prob. Divide each value of the array by sum of the array. Where we have marked most of the values as 0.\n",
    "\n",
    "\n",
    "Step 3\n",
    "* `np.random.choice`: We should use slight randomness when choosing the next word. If you don’t, the predictions can fall into a loop of the same words. Select top-k maximum-value probabilities as we have done in *Step 3* and then choose randomly from those k probabilities.<br> As a way to introduce slight randomness. The actual representation of the function is `np.random.choice(np.range(vocab_size), Length_of_result, probability(vocab_size))`. This will return the position of the character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    # remove 1 dimension\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    # Return as a scalar\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* `model`: Load the model with generated checkpoint\n",
    "* `new_state`: Initialize the state for prediction process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the output prediction we have two part.<br>\n",
    "**Learn the** `new_state` **values from given text.**<br>\n",
    "```\n",
    "new_state = sess.run(model.initial_state)\n",
    "for c in prime:\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0,0] = vocab_to_int[c]\n",
    "    feed = {model.inputs: x,\n",
    "            model.keep_prob: 1.,\n",
    "            model.initial_state: new_state}\n",
    "    preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                 feed_dict=feed)\n",
    "```\n",
    "Once we learned or identified the `state` values based on given text. \n",
    "\n",
    "**Apply the learned** `new_state` **values for prediction.**<br>\n",
    "```\n",
    "for i in range(n_samples):\n",
    "    x[0,0] = c\n",
    "    feed = {model.inputs: x,\n",
    "            model.keep_prob: 1.,\n",
    "            model.initial_state: new_state}\n",
    "    preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                 feed_dict=feed)\n",
    "\n",
    "    c = pick_top_n(preds, len(vocab))\n",
    "    samples.append(int_to_vocab[c])\n",
    "```\n",
    "For this process the last `pred` value need to be passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # Load the model from trained checkpoints\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            # Make the input as 2-dimentional array. Because we have trained the model with 2-dimentional array.\n",
    "            # Another reason is that the batch_size and n_steps we have set to 1, 1. Because we are  checkign with one charcter.\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            # Prediction comes as 2 dimentional array. With each character probability.\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "            \n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        \n",
    "        samples.append(int_to_vocab[c])\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: (1, 1)\n",
      "Shape of the output tensor: (1, 1)\n",
      "ALL LSTM cells creation: <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x11e2a88d0>\n",
      "Farrill.\n",
      "\n",
      "Anna had said that he had been to despressed his father in sicciant,\n",
      "and they was all the conversation of a men, as he was so so it\n",
      "was, and trousted it, and went to saying the sensior, and a letser.\n",
      "\n",
      "\"What did you can't take moment and that it is already between, this\n",
      "so so as it's nothing, and she could said that there has not\n",
      "becieed a mother of themself a supper of a simplity of moment, that\n",
      "it seould thought the better to have been to do.\n",
      "\n",
      "\"Ah he said in my wife if it.\"\n",
      "\n",
      "\"You do not see the corter and shates of him and tell you this will\n",
      "be directon. He'r always say, I, and he had not been and to take him\n",
      "out of his subject.\n",
      "\n",
      "\"What is them?\"\n",
      "\n",
      "\"I see it way, and see you made his bright, he did nit to be\n",
      "atcanding, I don't want to do as she had tore him and they wan\n",
      "it on the peasant.\"\n",
      "\n",
      "\"This movem in the what that seemed. Then when you have any ordiners of\n",
      "already to do as though I should not be all so in the sears on the\n",
      "sand that his father arranged with misiday of the moth\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: (1, 1)\n",
      "Shape of the output tensor: (1, 1)\n",
      "ALL LSTM cells creation: <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x124a2e550>\n",
      "Farnty heard the shilf the ponsing,\" has is\n",
      "asperid in was, he wint sond with the somete the mares, a poore of her hould baking tomerst of the soress on she with all\n",
      "corsianss to a tome and a denst the sampingsted to him hand to the sacking and aloned, his stared andowers. And somentens. He dentting his\n",
      "asfored and were\n",
      "sond, with the concoust with though, as im she as this shertion.\"\n",
      "\n",
      "\"Yes!\" see told, the canden's bace.... An the momere sone that\n",
      "thain as aspingity of she wale, as here to her, and \n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 500, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: (1, 1)\n",
      "Shape of the output tensor: (1, 1)\n",
      "ALL LSTM cells creation: <tensorflow.contrib.rnn.python.ops.core_rnn_cell_impl.MultiRNNCell object at 0x125836f98>\n",
      "Farcle it he had sure all the\n",
      "sortial fars at his beating and all\n",
      "sice in\n",
      "his ferinias to his seeming. She was it\n",
      "to a marriance when the moter of a motern, and was that he had took a\n",
      "sport that when he was notelly, the possion of the mear and\n",
      "strent, but there was so surd, and so that we had see that sent he her antoned to see him\n",
      "of the care off the pastion, with him with her and the cold the heard with the horry.\n",
      "\n",
      "\"Why do you talk a tone?\" She said.\n",
      "\n",
      "\"Why say a componity of to me, and you sould he doing to the\n",
      "seet, he wented,\" said Stepan Arkadyevitch.\n",
      "\n",
      "\"Yas've been to both. To seen is that they had been tried\n",
      "it, and he were stoll as so the stire. When she was somethorg anything, and\n",
      "that I she was so so thour hell.\"\n",
      "\n",
      "\"Yas, to something that he cauraked to\n",
      "him. It are merted befare them to be to his bate all,\n",
      "and she can't be in sence in the party, we had and deem so it of the\n",
      "countent of marry of all, as he had treed that in him.\"\n",
      "\n",
      "\"Yes,\" she said to time her for mer, were\n",
      "all one of\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:dlnd]",
   "language": "python",
   "name": "conda-env-dlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {
    "height": "264px",
    "width": "222px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
