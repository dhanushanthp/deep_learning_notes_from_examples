{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "f1a822c3-3577-4ba5-ac78-8c778e640fab"
    }
   },
   "source": [
    "# Steps\n",
    "**Read file and encoding**\n",
    "* Read the input file.\n",
    "* Select unique characters or classes from full text as vocub.\n",
    "* Encoding: Generate number for each classes or characters.\n",
    "* Replace all text with numbers using generated integers-character mapping.\n",
    "\n",
    "[**Create Sequence for RNN input**](#create_sequence)\n",
    "* Convert the encoded(number replaced text) text as sequences\n",
    "* Use the `batch_size` and `number_of_steps` for mini batch\n",
    "* Create a total batch with mini batch, This will have K number of main batch\n",
    "\n",
    "[**Initialization of TF**](#tf_initialize)\n",
    "* Initialize or define `input`, `output` and `keep_prob` for RNN\n",
    "* Initialize or define Single cell LSTM\n",
    "* Initialize Multicell or Multilayer LSTM using a single\n",
    "* Initialize Output and State of LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "1c20d9b7-da1b-43be-8267-25ee58ea85eb"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "9268bf96-b50e-4fef-a671-78bc9237d046"
    }
   },
   "source": [
    "# Input Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "653daae6-8021-4b31-a2aa-7b970961991c"
    }
   },
   "source": [
    "Read the file as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "c34b833c-470c-4db7-962c-57cde0106ba4"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g was in confusion in the Oblonskys' house. The wife had\n",
      "discovered that the husband was carrying on\n"
     ]
    }
   ],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text=f.read()\n",
    "print(text[100:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "5ded74c4-b853-4a27-86f6-01909e5a61d5"
    }
   },
   "source": [
    "Create unique characters from total text as **vocabulary**. This helps to decide the **number of classes** or characters in a specific text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "1e73f3cb-73d8-4d8b-b273-34b1ecef31fd"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "b05da087-8ec9-44a5-89b2-6283d64c7b6b"
    }
   },
   "source": [
    "Map letters with numbers. Because our Network understand only the numbers not the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "96721dd0-e55d-4f13-bf1a-07c6f035b25c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2': 17, '/': 14, 'Y': 53, '5': 20, '!': 2, 'c': 59, '&': 6, 'e': 61, 'I': 37, 'O': 43, 'H': 36, 'Q': 45, 'h': 64, 'A': 29, 'b': 58, '(': 8, 'X': 52, 'J': 38, 'z': 82, 'v': 78, 'n': 70, ' ': 1, \"'\": 7, '3': 18, '\"': 3, 'R': 46, '.': 13, '*': 10, '\\n': 0, '1': 16, '0': 15, 'T': 48, '8': 23, 'S': 47, 'r': 74, '7': 22, 'N': 42, 's': 75, ':': 25, 'Z': 54, 'M': 41, 'F': 34, '?': 27, '6': 21, 'x': 80, ')': 9, 'm': 69, 'G': 35, '`': 56, 'C': 31, 'd': 60, 'w': 79, 'W': 51, '4': 19, 'p': 72, 'i': 65, 'q': 73, 'f': 62, 'K': 39, 'E': 33, 'V': 50, 'y': 81, '9': 24, 'u': 77, 'D': 32, '_': 55, 'U': 49, ',': 11, 'a': 57, '@': 28, 'l': 68, ';': 26, 'j': 66, '-': 12, 'k': 67, '$': 4, '%': 5, 't': 76, 'L': 40, 'B': 30, 'o': 71, 'P': 44, 'g': 63}\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "print(vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "b5fd3b28-cfaf-444e-8fdf-b55836575586"
    }
   },
   "source": [
    "Using the mapping convert all letters as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "266fb8ae-b313-4749-b231-3b23db791b1a"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g was in confusion in the Oblonskys' house. The wife had\n",
      "discovered that the husband was carrying on\n",
      "[63  1 79 57 75  1 65 70  1 59 71 70 62 77 75 65 71 70  1 65 70  1 76 64 61\n",
      "  1 43 58 68 71 70 75 67 81 75  7  1 64 71 77 75 61 13  1 48 64 61  1 79 65\n",
      " 62 61  1 64 57 60  0 60 65 75 59 71 78 61 74 61 60  1 76 64 57 76  1 76 64\n",
      " 61  1 64 77 75 58 57 70 60  1 79 57 75  1 59 57 74 74 81 65 70 63  1 71 70]\n"
     ]
    }
   ],
   "source": [
    "# Map letter with numbers, Inverse of integer mapping\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)\n",
    "print(text[100:200])\n",
    "print(encoded[100:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "4b3f5c80-6485-487b-8026-e5f1ec553547"
    }
   },
   "source": [
    "# Convert inputs as batch sequences <a class=\"anchor\" id=\"create_sequence\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "9575f2cb-8ee4-49fa-8d34-b9fe40aeffdd"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64; n_steps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "3793495a-a59e-490f-966d-053db743fa8c"
    }
   },
   "source": [
    "Collect Characters per Sequence or mini Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "e117f775-9c0d-496a-9bcc-f8c3b4fcd328"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n"
     ]
    }
   ],
   "source": [
    "chars_per_batch = batch_size * n_steps\n",
    "print(chars_per_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "715e8088-1f03-4ad0-9373-ebdd7c16e96a"
    }
   },
   "source": [
    "Total Number of Batches, Once we devide the total chars per mini batch, We able to get the Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "c6785600-abe3-4d66-8c5c-f28cb95085f9"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620\n"
     ]
    }
   ],
   "source": [
    "n_batchers = len(encoded)//chars_per_batch\n",
    "print(n_batchers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "5c16c37a-ea0e-43cd-b094-ce59894fa8a0"
    }
   },
   "source": [
    "Based on possible Batch size. Select charactors that can create **full mini batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "af0ccaee-61c0-4203-8989-bd8e96a9e246"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_text_length: 1985223, new_text_length: 1984000\n"
     ]
    }
   ],
   "source": [
    "new_encoded = encoded[: chars_per_batch * n_batchers]\n",
    "print('old_text_length: {}, new_text_length: {}'.format(len(encoded), len(new_encoded)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "bfd16fc8-574e-46da-8ff5-fbe0a0e17ab1"
    }
   },
   "source": [
    "Reshape the batch according to mini batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "e163a45b-362b-4e28-81e8-892f3af51523"
    }
   },
   "outputs": [],
   "source": [
    "new_encoded = new_encoded.reshape((batch_size, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "b151fcf8-82d4-4503-9663-fd686cd1e7cd"
    }
   },
   "source": [
    "Create mini batch with number of steps. In otherwords, How many steps we are going to look back to create RNN or LSTM.\n",
    "\n",
    "The `yield` function behave as `return`, Then reason we use `yeild` is, This won't keep the data in memory. When ever we need this will get executed and return the values.\n",
    "\n",
    "To create input and output, We move the output by 1 step. Where if the input in $i^{th}$ position, Then the output will be $(i+1)^{th}$ position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "77d3c867-6c51-474f-9775-e75b458b1d3f"
    }
   },
   "outputs": [],
   "source": [
    "def get_batch(new_encoded, n_steps):\n",
    "    for n in range(0, new_encoded.shape[1], n_steps):\n",
    "        x = new_encoded[:, n: n+ n_steps]\n",
    "        y_temp = new_encoded[:, n+1:n+n_steps+1]\n",
    "        y = np.zeros(x.shape, dtype=x.dtype)\n",
    "        y[:,:y_temp.shape[1]] = y_temp\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "358de146-0e21-4278-b6a0-eca62cb6f4e1"
    }
   },
   "outputs": [],
   "source": [
    "batch = get_batch(new_encoded, n_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "7876ecd4-ac44-4106-9b35-997f39c07222"
    }
   },
   "source": [
    "Get each value from `generator` Object by using `next` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "ebd56c80-3e74-406c-b301-dc02201a8133"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31 64 57 ...,  1 77 70]\n",
      " [70  1 76 ..., 81  1 69]\n",
      " [70 71 78 ..., 59 57 68]\n",
      " ..., \n",
      " [70 60  1 ..., 78 61 74]\n",
      " [11  0 71 ...,  1 57 70]\n",
      " [ 1 70 71 ..., 70 71 76]]\n",
      "[[64 57 72 ..., 77 70 64]\n",
      " [ 1 76 71 ...,  1 69 65]\n",
      " [71 78 65 ..., 57 68  1]\n",
      " ..., \n",
      " [60  1 75 ..., 61 74  1]\n",
      " [ 0 71 74 ..., 57 70 81]\n",
      " [70 71 79 ..., 71 76 64]]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(batch)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "b3256727-066e-421e-bfae-ce7e4ce19cd3"
    }
   },
   "source": [
    "# Creation of Tensorflow variables. <a class=\"anchor\" id=\"tf_initialize\"></a>\n",
    "## Input Variables\n",
    "Initialize `inputs`, `outputs` and `keep_prob` probability in hidden units (hidden neurons in LSTM cell). The size of the input and output is $batch\\_size \\times n\\_step$. Because we are always going to provide the input as mini batch as well as tensors. Here `keep_prob` is a scaler and 0 dimentional tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "cfae01b0-d0d1-40a7-b2dc-21139a39c907"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.int32, shape=[batch_size, n_steps], name='inputs')\n",
    "targets = tf.placeholder(tf.int32, shape=[batch_size, n_steps], name='targets')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "80d26f0b-8a07-413c-8548-253a0e25e8c6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"inputs:0\", shape=(64, 50), dtype=int32)\n",
      "Tensor(\"targets:0\", shape=(64, 50), dtype=int32)\n",
      "Tensor(\"keep_prob:0\", dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(targets)\n",
    "print(keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "b067d10c-e91a-4d16-bcb5-ae141acc7d72"
    }
   },
   "outputs": [],
   "source": [
    "lstm_size = 128 # Number of hidden units in LSTM cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "9912555f-d6ff-4e67-9783-98bdd7743bf6"
    }
   },
   "source": [
    "1. Create a single LSTM cell using `tf.contrib.rnn.BasicLSTMCell(lstm_size)`. From single cell we can create a multi Layer LSTM cells using `MultiRNNCell` Function.\n",
    "2. We need to Initialize the cell states with zero. Here the state is the value that pass to another state(from time `t` to time `t+1`). When we create the states with cells. That will automatically create for hidden LSTM cells as well. This helps to pass the information to next state to remember things. \n",
    "\n",
    "We have 2 hidden layers or 2 LSTM cells. We have used a function to generate 2 LSTM cells dynamically. Initially I have tried this with by multiplying list with a value. `[cell] * number_of_layers`. Because If we multiply the same cell then we are just reusing it. But we need to create a new cell. Therefore this has given a error while creating a RNN Layer with `tf.nn.dynamic_rnn`. Therefore created a function to generate each cell. Here we don't pass any data. We are just defineing the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "c41aa566-d921-4e59-a58e-822ca8a23154"
    }
   },
   "outputs": [],
   "source": [
    "def create_single_cell(lstm_size, keep_prob):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "9cd1311c-f018-4622-9b80-42d794e87e23"
    }
   },
   "outputs": [],
   "source": [
    "number_of_layers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "7788ef8c-ba40-45c8-b345-f0ae7807855a"
    }
   },
   "source": [
    "Stack the LSTM cell in to Mutliple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "bc155f6d-6192-4c14-84f4-702198961f3b"
    }
   },
   "outputs": [],
   "source": [
    "cells = tf.contrib.rnn.MultiRNNCell([create_single_cell(lstm_size, keep_prob) for _ in range(number_of_layers)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create initial state for each cell. The `cell.zero_state` function will create initial values for states. Since initial input size at time t is `Number of sequence` or `batch_size`. So each cell will be created with `batch_size` of initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "59f28ccc-903e-4221-a0ce-5a9bb119a42a"
    }
   },
   "outputs": [],
   "source": [
    "# values that pass to time-steps\n",
    "initial_state = cells.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "116fd266-9aeb-4139-9693-2f5dc473e193"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x181bd2c438>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(64, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(64, 128) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState_1/BasicLSTMCellZeroState/zeros:0' shape=(64, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState_1/BasicLSTMCellZeroState/zeros_1:0' shape=(64, 128) dtype=float32>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(cells)\n",
    "initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "4c294480-88bc-4996-aa21-aad253eb8545"
    }
   },
   "source": [
    "# Creation of RNN Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "905a3bc8-16f5-43d1-aaef-f9bcb7877a34"
    }
   },
   "source": [
    "The class of `one_hot` encoder is `vocub`: Unique characters from whole text. The number of classes helps to find the probability of each letter by creating `logits`, The logit is a vector for each character after applying softmax functions. Because when we are passing a single character we will pass a vector, where the specific character will marked as 1 amoung a vector of classes. And from RNN output the character comes with probability distribution from softmax function. This will be input for each LSTM cell as vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "99fd5c4f-b406-4a96-a02d-10c000b8d569"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes or charactors\n",
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print('Classes or charactors')\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "766810cd-a6d9-4cd9-9223-318d91fcd83e"
    }
   },
   "source": [
    "Once we pass through `one_hot`, It show a 3D result of input. Where\n",
    "* 64: `batch_size`\n",
    "* 50:`number_of_steps`\n",
    "* 83: `classes`\n",
    "\n",
    "Because the `tf.one_hot` converts the each character vector size of 83. We normally we call this process as one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "4743498d-81b2-4aa9-8c0b-7988e3d6ecf5"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 83\n",
      "Tensor(\"one_hot:0\", shape=(64, 50, 83), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(vocab)\n",
    "print('number of classes: ' + str(num_classes))\n",
    "x_one_hot = tf.one_hot(inputs, num_classes)\n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "c2dd0dc1-0e73-47f2-b646-735b090dd09e"
    }
   },
   "source": [
    "Create a RNN architecture by combining all initialized cell or LSTM Multilayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "73e3d222-4fc0-470a-8797-78102e1ce337"
    }
   },
   "source": [
    "* **Output**: This is the actual output from LSTM cell.\n",
    "* **State**: This is a output from LSTM cell that pass to next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "bc2b25b5-9310-48b1-970b-9963164c3b2b"
    }
   },
   "outputs": [],
   "source": [
    "outputs, state = tf.nn.dynamic_rnn(cells, x_one_hot, initial_state=initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "0f5b7b9c-937d-4c4a-97e6-51b737f6d7be"
    }
   },
   "outputs": [],
   "source": [
    "final_state= state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "bf6d41cc-9713-4d81-8515-3564c426778f"
    }
   },
   "source": [
    "Now we have create a RNN network with 2 hidden layer or 2 LSTM cells. And each character input has been encoded with One Hot Encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "b4ea2a87-c0f7-4069-9d97-77bcfc027042"
    }
   },
   "source": [
    "# Reshape Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "0f7c7c14-caa3-456f-914c-3cd7400fe3d3"
    }
   },
   "source": [
    "<img src=\"assets/charRNN.png\" width=500px>\n",
    "\n",
    "We have $N$ steps in mini match. \n",
    "- So we are going to get $N$ outputs from our RNN. \n",
    "\n",
    "At the same time we are passing multiple sequence $M$ in parallel. \n",
    "- So we get another set of output for each sequence. So the Total output will be $M \\times N$\n",
    "\n",
    "LSTM also has $L$ number of hidden units.\n",
    "- So the total output from LSTM is 3 dimentional. Which is $M \\times N \\times L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "7479cef3-b531-4268-93ac-fee3020f640d"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose:0' shape=(64, 50, 128) dtype=float32>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we will get LSTM outputs as `list`. So the list need to concatenated to create an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "19e2d2ba-8102-4740-a775-46636b10382f"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_2:0' shape=(64, 50, 128) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_output = tf.concat(outputs, axis=1)\n",
    "seq_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "a65db0ac-9e5f-45cd-9faa-70c85933abfd"
    }
   },
   "source": [
    "All the outputs from LSTM are going through the **same sigmoid** layer with the **same weights**. Therefore we can reshape this output from LSTM in to 2D tensor with the shape $(M * N) \\times L$. That is, \n",
    "* Each row output for **each charactor**. Where the total charactors are $M*N$. In other words **each row** is **one step** and **one sequence**. Or Every row is one output from one LSTM cell.\n",
    "* $M*N$ rows with $L$ columns, Where $charactors\\ per\\ mini\\ batch = M * N$\n",
    "\n",
    "Then the array need to reshaped as we have discussed above. Where each row for each charactor with $L$ number of columns.\n",
    "\n",
    "Actually this reshape will helps with Softmax function matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "2e11aeaa-3294-44d3-8e62-28bb18cb9d61"
    }
   },
   "outputs": [],
   "source": [
    "output_reshape = tf.reshape(seq_output, [-1, lstm_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "016f4fb3-ff07-4aae-9236-1c8026d99646"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(3200, 128) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "716b0e10-94e3-4a0b-9e25-79a2dab64d21"
    }
   },
   "source": [
    "Once we have reshaped. Then we can create the weights and bias for the softmax layer. Then we will do the matrix multiplication and adding bias to get the logits. Then we will pass the logits through the softmax function.\n",
    "\n",
    "The weights and bias need to be created with ```python tf.variable_scope('name_for_softmax')```. Because RNN cell also have weights and bias with the default. So If we define softmax weight and bias without `tf.variable_scope`. Then this will take the default name as RNN. And that will throw an error. To avoid the error we pass a different name for the softmax function weights and the bias.\n",
    "\n",
    "<img src=\"assets/softmaxcell.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "a37d7f7d-c89b-4d14-a215-eb5ff3ed3ca8"
    }
   },
   "source": [
    "The shape of the weights will be Number of hidden units in a LSTM cell and the size of class/unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "516fb0d9-3431-41e3-81b8-e43a6d8b10b8"
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax_variables'):\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "bc5e1c34-3d6a-44c5-988a-9e1046ebd5f0"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'softmax_variables_1/Variable:0' shape=(128, 83) dtype=float32_ref>\n",
      "<tf.Variable 'softmax_variables_1/Variable_1:0' shape=(83,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(softmax_w)\n",
    "print(softmax_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "84f5461e-7a98-471e-ab25-72f4f91b4895"
    }
   },
   "outputs": [],
   "source": [
    "prediction = tf.matmul(output_reshape, softmax_w) + softmax_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "bb5339ca-5b20-4fcd-99df-9c6fbad9f730"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3200, 83) dtype=float32>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "fdc39824-d4b1-4436-a52d-73446975535c"
    }
   },
   "source": [
    "Now we have logits for each charactor in mini batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "42ac60df-ddfc-4c28-8f2f-102a571422d0"
    }
   },
   "source": [
    "Apply the softmax function for output logits from each LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "b2a9b0c6-d01a-4441-b203-fcb771387bda"
    }
   },
   "outputs": [],
   "source": [
    "logits = tf.nn.softmax(prediction, name='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "5b59adef-59da-4ebe-882d-114df22d8c15"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'predictions:0' shape=(3200, 83) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "16621169-c162-4b69-b795-d231a97e731f"
    }
   },
   "source": [
    "# Training loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "79e92abf-4682-4ffb-9d76-5e79db3bc3d6"
    }
   },
   "source": [
    "Here we will be using `cross entrophy` loss to find the error loss. The tensorflow function support with `tf.nn.softmax_cross_entropy_with_logits` then find the mean to get the loss actual loss.\n",
    "\n",
    "The **targets need to be reshaped in to 2D tensor** as we have done with RNN outputs. Basically the logits and the targets need to be in the same shape. logits is the one that come throgh softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'targets:0' shape=(64, 50) dtype=int32>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "cc674c23-74c1-4103-818d-5e8ab28f5e7f"
    }
   },
   "outputs": [],
   "source": [
    "y_one_hot = tf.one_hot(targets, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "34aa5871-98e4-4a64-be3e-5c6e67e2b2e3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'one_hot_2:0' shape=(64, 50, 83) dtype=float32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "5d1aefb8-7f02-4dff-b9ef-b5205442fbc6"
    }
   },
   "outputs": [],
   "source": [
    "y_out_reshape = tf.reshape(y_one_hot, logits.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "974b2106-4ccb-4006-bd0d-7609de6488af"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_1:0' shape=(3200, 83) dtype=float32>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_out_reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "ad68b441-c231-43f5-9741-55bd811f4e6b"
    }
   },
   "source": [
    "Calculate the loss: Return a probability distribution for each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "e06d1afe-77c1-44a3-9036-bb4d7d33c08e"
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_out_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "f62d017c-0301-4364-ada4-1fd760938f06"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_4:0' shape=(3200,) dtype=float32>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "b178af12-be86-4f5e-b712-a63a74d67abd"
    }
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "f9a43d65-afcb-4062-a3c5-06e4114ce991"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "12957753-382b-4061-8dfe-e687a5eea5f9"
    }
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "bb888d3c-f3fe-417b-973d-cc57904c1c78"
    }
   },
   "source": [
    "Here we build the optimizer. Normal RNNs has issues with gradients exploding and disappearing. LSTMs fix the disappearance problem, but the gradients can still grow without bound. To fix this, we can clip the gradients above some threshold. That is, if a gradient is larger than that threshold, we set it to the threshold. This will ensure the gradients never grow overly large. Then we use an AdamOptimizer for the learning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "665bd841-4a92-4a2c-b355-83c3a0ff471f"
    }
   },
   "source": [
    "Map all the variables that we have assigned with this network. And this is handled by the Graph of RNN. To reset this mapping or graph we need to use `tf.reset_default_graph()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "a7963783-3aa8-468b-9809-c8bd86347a1b"
    }
   },
   "outputs": [],
   "source": [
    "grad_clip = 5;\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "d762134d-380c-4010-a53d-a1f5e72b1c49"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer for training, using gradient clipping to control exploding gradients\n",
    "tvars = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "b838c7a0-f99b-4a75-85f8-844e25893b82"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(211, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_variables/Variable:0' shape=(128, 83) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_variables/Variable_1:0' shape=(83,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_variables_1/Variable:0' shape=(128, 83) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_variables_1/Variable_1:0' shape=(83,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "205c079b-add4-479c-8103-2c3b2c5f6198"
    }
   },
   "outputs": [],
   "source": [
    "grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "c26f54a1-6b56-4295-8741-7bf6701345f6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_0:0' shape=(211, 512) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(256, 512) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(512,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(128, 83) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(83,) dtype=float32>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "e1df5ae6-8ff5-4b18-851f-2ed94d98cc1e"
    }
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "a38edcc0-fd66-4fcf-8d20-d9b5389c43d9"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.adam.AdamOptimizer at 0x1a1e1c2358>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "136b3b5c-4eb2-4f05-bc57-a7a59619d235"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = train_op.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "86d3f44a-0fb6-44c2-a2a3-01b1f38ff26d"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'Adam' type=NoOp>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "e3b04f4d-7320-4947-a1ea-b3718fbed4de"
    }
   },
   "source": [
    "# Build the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "nbpresent": {
     "id": "3f78056e-d8cc-4605-b8a1-06394390949f"
    }
   },
   "source": [
    "To actually run data through the LSTM cells, we will use [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/nn/dynamic_rnn). This function will pass the hidden and cell states across LSTM cells appropriately for us. It returns the outputs for each LSTM cell at each step for each sequence in the mini-batch. It also gives us the final LSTM state. We want to save this state as `final_state` so we can pass it to the first LSTM cell in the the next mini-batch run. For `tf.nn.dynamic_rnn`, we pass in the cell and initial state we get from `build_lstm`, as well as our input sequences. Also, we need to one-hot encode the inputs before going into the RNN."
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
