{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "vocab = sorted(set(text))\n",
    "vocab_to_int = {c: i for i, c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "\n",
    "# Convert all the text as integer\n",
    "encoded = np.array([vocab_to_int[c] for c in text], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mini batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, n_steps):\n",
    "    # Get the total number of characters per batch\n",
    "    chars_per_batch = batch_size * n_steps\n",
    "\n",
    "    # Total Number of batchs\n",
    "    n_batches = len(arr) // chars_per_batch\n",
    "\n",
    "    # Get the full pack of charactors for each batch\n",
    "    arr = arr[:n_batches * chars_per_batch]\n",
    "\n",
    "    # Reshape the array by batch_size, Horizonal reshaping based on batch size.\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "\n",
    "    # Create steps from mini batch, Keep in mind this is a matrix\n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        x = arr[:, n:n + n_steps]\n",
    "\n",
    "        # At last batch the y will be sort by 1 charactor. That will create error in `tf.nn.dynamic_rnn` for array size mis match\n",
    "        y_temp = arr[:, n + 1:n + n_steps + 1]\n",
    "        \n",
    "        # Create a zero array and append with y output\n",
    "        y = np.zeros(x.shape, dtype=np.int32)\n",
    "        y[:, :y_temp.shape[1]] = y_temp\n",
    "        \n",
    "        # To handle memory\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[31 64 57 72 76]\n",
      " [81 11  3  1 57]\n",
      " [57 63 70 65 62]\n",
      " [ 1 57  1 72 74]\n",
      " [61  1 76 64 57]\n",
      " [ 1 75 57 79  1]\n",
      " [59 76 61 60  1]\n",
      " [57 75  1 75 77]\n",
      " [61 60  1 65 76]\n",
      " [59 61  1 71 62]]\n",
      "[[64 57 72 76 61]\n",
      " [11  3  1 57 70]\n",
      " [63 70 65 62 65]\n",
      " [57  1 72 74 61]\n",
      " [ 1 76 64 57 70]\n",
      " [75 57 79  1 64]\n",
      " [76 61 60  1 79]\n",
      " [75  1 75 77 62]\n",
      " [60  1 65 76 75]\n",
      " [61  1 71 62  1]]\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "batch_size = 10; n_steps=5\n",
    "batchs = get_batches(encoded, batch_size, n_steps)\n",
    "x, y = next(batchs)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, n_steps):\n",
    "    inputs = tf.placeholder(\n",
    "        shape=[batch_size, n_steps], dtype=tf.int32, name='inputs')\n",
    "    \n",
    "    outputs = tf.placeholder(\n",
    "        shape=[batch_size, n_steps], dtype=tf.int32, name='outputs')\n",
    "    \n",
    "    keep_prob = tf.placeholder(dtype=tf.float32, name='keep_prob')\n",
    "    \n",
    "    print('Shape of the input tensor: {}'.format(inputs.get_shape()))\n",
    "    print('Shape of the output tensor: {}'.format(outputs.get_shape()))\n",
    "    \n",
    "    return inputs, outputs, keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "\n",
    "    # Single cell creation\n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "\n",
    "    # Initialize different cells by calling single cell creation function\n",
    "    cells = tf.contrib.rnn.MultiRNNCell(\n",
    "        [build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    print('ALL LSTM cells creation: {}'.format(cells))\n",
    "\n",
    "    # Values that pass to another cell horizontally for memory or time-steps\n",
    "    initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cells, initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, lstm_size, number_of_classes):\n",
    "    # lstm_output comes as list. Concat to create as array\n",
    "    pre_output = tf.concat(lstm_output, axis=1)\n",
    "\n",
    "    # Reshape the output matrix as [batch_size*n_steps X lstm_size]\n",
    "    # Each row output for each charactor. The number of rows will batch_size*n_steps.\n",
    "    output = tf.reshape(pre_output, [-1, lstm_size])\n",
    "\n",
    "    # Apply softmax function by creation a softmax layer\n",
    "    # `variable_scope` helps to change the default name for weight initialization.\n",
    "    #  Because RNN cell also have weights and bias with the default\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(\n",
    "            tf.truncated_normal((lstm_size, number_of_classes), stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(number_of_classes))\n",
    "\n",
    "    # Input for softmax function\n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "    predictions = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return predictions, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_of_classes):\n",
    "\n",
    "    # convert the numbers to one hot encoding\n",
    "    target_one_hot = tf.one_hot(targets, num_of_classes)\n",
    "\n",
    "    # Change the shape similer to output from RNN\n",
    "    target = tf.reshape(target_one_hot, logits.get_shape())\n",
    "\n",
    "    # Apply cross entrophy\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=target)\n",
    "    \n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer or Simple gradient decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `tf.trainable_variables`: weights and bias used during the training, This is assigned by TF\n",
    "\n",
    "* `tf.gradients`: Constructs symbolic partial derivatives of sum of ys w.r.t. x in xs. That return $\\delta w, \\delta b = tf.gradients(cost, [W, b])$. The `tf.gradients()` returns the gradient of cost wrt each tensor in the second argument as a list in the same order.\n",
    "\n",
    "* `tf.clip_by_global_norm` is a function to put the upper bound on the gradient value wrt to each variables. `grad_clip` is the upper bound.\n",
    "\n",
    "* AdamOptimizer is used to minimise the prediction error. It is variation of gradient descent. \n",
    "Here's a list on optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    # weights and bias used during the training, This is assigned by TF\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    \n",
    "    # Tune the weights\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the class of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    def __init__(self, num_classes, batch_size=64, num_steps = 50, lstm_size=128, num_layers = 2, learning_rate=0.001,\n",
    "                grad_clip=5, sampling=False):\n",
    "\n",
    "        if sampling==True:\n",
    "            batch_size, num_steps = 1,1\n",
    "\n",
    "        # Reset all variabled that has mapped with this model\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Tensor input\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "        \n",
    "        # Tensor cells and state.\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "        \n",
    "        # Encode the inputs.\n",
    "        input_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # Create a RNN network with LSTM cell, input and initial state\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, input_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        \n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training parameters\n",
    "* `batch_size`    - Number of sequences running through the network in one pass.\n",
    "* `num_steps`     - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lstm_size`     - The number of units in the hidden layers.\n",
    "* `num_layers`    - Number of hidden LSTM layers to use\n",
    "* `learning_rate` - Learning rate for training\n",
    "* `keep_prob`     - The dropout keep probability when training. If you're network is overfitting, try decreasing this.\n",
    "\n",
    "## Tips and Tricks\n",
    "\n",
    "### Monitoring Validation Loss vs. Training Loss\n",
    "The most important quantity to keep track of is the difference between your **training loss** (printed during training) and the **validation loss** (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "- If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to **decrease your network size, or to increase dropout**. For example you could try dropout of 0.5 and so on.\n",
    "- If your training/validation loss are about equal then your model is **underfitting**. **Increase the size of your model (either number of layers or the raw number of neurons per layer)**\n",
    "\n",
    "### Approximate number of parameters\n",
    "\n",
    "The two most important parameters that control the model are `lstm_size` and `num_layers`. I would advise that you always use `num_layers` of either 2/3. The `lstm_size` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "- The number of parameters in your model. This is printed when you start training.\n",
    "- The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    "These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "- I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `lstm_size` larger.\n",
    "- I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "### Best models strategy\n",
    "\n",
    "The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). **Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.**\n",
    "\n",
    "It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end **take whatever checkpoint gave the best validation performance**.\n",
    "\n",
    "By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100        # Sequences per batch\n",
    "num_steps = 100         # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001   # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: (100, 100)\n",
      "Shape of the output tensor: (100, 100)\n",
      "ALL LSTM cells creation: <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fbc8680a320>\n",
      "WARNING:tensorflow:From <ipython-input-10-62af02acb06c>:11: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "# Print losses every N interations\n",
    "print_every_n = 50\n",
    "\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "\n",
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "# Store the model for best validation performance, Where the validation loss is low.\n",
    "saver = tf.train.Saver(max_to_keep=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20...  Training Step: 50...  Training loss: 3.1663...  0.3197 sec/batch\n",
      "Epoch: 1/20...  Training Step: 100...  Training loss: 3.0855...  0.3228 sec/batch\n",
      "Epoch: 1/20...  Training Step: 150...  Training loss: 2.7331...  0.3211 sec/batch\n",
      "Epoch: 2/20...  Training Step: 200...  Training loss: 2.4193...  0.3217 sec/batch\n",
      "Epoch: 2/20...  Training Step: 250...  Training loss: 2.3242...  0.3211 sec/batch\n",
      "Epoch: 2/20...  Training Step: 300...  Training loss: 2.2082...  0.3230 sec/batch\n",
      "Epoch: 2/20...  Training Step: 350...  Training loss: 2.1719...  0.3245 sec/batch\n",
      "Epoch: 3/20...  Training Step: 400...  Training loss: 2.0444...  0.3230 sec/batch\n",
      "Epoch: 3/20...  Training Step: 450...  Training loss: 1.9644...  0.3228 sec/batch\n",
      "Epoch: 3/20...  Training Step: 500...  Training loss: 1.9265...  0.3210 sec/batch\n",
      "Epoch: 3/20...  Training Step: 550...  Training loss: 1.8887...  0.3217 sec/batch\n",
      "Epoch: 4/20...  Training Step: 600...  Training loss: 1.7832...  0.3210 sec/batch\n",
      "Epoch: 4/20...  Training Step: 650...  Training loss: 1.7954...  0.3223 sec/batch\n",
      "Epoch: 4/20...  Training Step: 700...  Training loss: 1.7485...  0.3243 sec/batch\n",
      "Epoch: 4/20...  Training Step: 750...  Training loss: 1.7314...  0.3212 sec/batch\n",
      "Epoch: 5/20...  Training Step: 800...  Training loss: 1.6814...  0.3226 sec/batch\n",
      "Epoch: 5/20...  Training Step: 850...  Training loss: 1.6486...  0.3229 sec/batch\n",
      "Epoch: 5/20...  Training Step: 900...  Training loss: 1.6204...  0.3205 sec/batch\n",
      "Epoch: 5/20...  Training Step: 950...  Training loss: 1.6005...  0.3231 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1000...  Training loss: 1.5867...  0.3219 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1050...  Training loss: 1.5922...  0.3202 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1100...  Training loss: 1.5521...  0.3220 sec/batch\n",
      "Epoch: 6/20...  Training Step: 1150...  Training loss: 1.5449...  0.3222 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1200...  Training loss: 1.4985...  0.3208 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1250...  Training loss: 1.5504...  0.3227 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1300...  Training loss: 1.4652...  0.3206 sec/batch\n",
      "Epoch: 7/20...  Training Step: 1350...  Training loss: 1.4674...  0.3230 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1400...  Training loss: 1.4758...  0.3207 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1450...  Training loss: 1.4351...  0.3215 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1500...  Training loss: 1.3947...  0.3228 sec/batch\n",
      "Epoch: 8/20...  Training Step: 1550...  Training loss: 1.3883...  0.3233 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1600...  Training loss: 1.3582...  0.3206 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1650...  Training loss: 1.3970...  0.3216 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1700...  Training loss: 1.3235...  0.3201 sec/batch\n",
      "Epoch: 9/20...  Training Step: 1750...  Training loss: 1.3480...  0.3239 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1800...  Training loss: 1.3801...  0.3228 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1850...  Training loss: 1.3283...  0.3219 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1900...  Training loss: 1.3374...  0.3206 sec/batch\n",
      "Epoch: 10/20...  Training Step: 1950...  Training loss: 1.3994...  0.3218 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2000...  Training loss: 1.3427...  0.3220 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2050...  Training loss: 1.2991...  0.3215 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2100...  Training loss: 1.2897...  0.3219 sec/batch\n",
      "Epoch: 11/20...  Training Step: 2150...  Training loss: 1.3059...  0.3224 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2200...  Training loss: 1.3033...  0.3214 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2250...  Training loss: 1.3135...  0.3224 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2300...  Training loss: 1.2382...  0.3216 sec/batch\n",
      "Epoch: 12/20...  Training Step: 2350...  Training loss: 1.2618...  0.3214 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2400...  Training loss: 1.2886...  0.3216 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2450...  Training loss: 1.2452...  0.3216 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2500...  Training loss: 1.2566...  0.3217 sec/batch\n",
      "Epoch: 13/20...  Training Step: 2550...  Training loss: 1.2599...  0.3206 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2600...  Training loss: 1.2215...  0.3221 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2650...  Training loss: 1.2577...  0.3212 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2700...  Training loss: 1.1976...  0.3222 sec/batch\n",
      "Epoch: 14/20...  Training Step: 2750...  Training loss: 1.2183...  0.3234 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2800...  Training loss: 1.2550...  0.3226 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2850...  Training loss: 1.2263...  0.3210 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2900...  Training loss: 1.2363...  0.3223 sec/batch\n",
      "Epoch: 15/20...  Training Step: 2950...  Training loss: 1.2670...  0.3214 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3000...  Training loss: 1.2272...  0.3216 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3050...  Training loss: 1.2127...  0.3219 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3100...  Training loss: 1.1704...  0.3221 sec/batch\n",
      "Epoch: 16/20...  Training Step: 3150...  Training loss: 1.1789...  0.3215 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3200...  Training loss: 1.1851...  0.3214 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3250...  Training loss: 1.2122...  0.3208 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3300...  Training loss: 1.1905...  0.3215 sec/batch\n",
      "Epoch: 17/20...  Training Step: 3350...  Training loss: 1.2037...  0.3211 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3400...  Training loss: 1.1934...  0.3216 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3450...  Training loss: 1.1866...  0.3218 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3500...  Training loss: 1.1835...  0.3214 sec/batch\n",
      "Epoch: 18/20...  Training Step: 3550...  Training loss: 1.1746...  0.3223 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3600...  Training loss: 1.1780...  0.3227 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3650...  Training loss: 1.1825...  0.3220 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3700...  Training loss: 1.1829...  0.3215 sec/batch\n",
      "Epoch: 19/20...  Training Step: 3750...  Training loss: 1.1389...  0.3215 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3800...  Training loss: 1.1454...  0.3214 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3850...  Training loss: 1.1509...  0.3205 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3900...  Training loss: 1.1830...  0.3211 sec/batch\n",
      "Epoch: 20/20...  Training Step: 3950...  Training loss: 1.1512...  0.3220 sec/batch\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initialize all TF variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Tuple of initial state with 0 tensors\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            \n",
    "            # we pass the each function element of list. That return tuples. So we pick from each.\n",
    "            # batch_loss: Previos iteration loss\n",
    "            # new_state: previous iteration final_state will be the new_state for next iteration.\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "\n",
    "    # i{iteration number}_l{# hidden layer units}.ckpt\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helps to pick the latest check point.\n",
    "tf.train.get_checkpoint_state('checkpoints')\n",
    "\n",
    "# Pick the model by it's name\n",
    "checkpoint = 'checkpoints/i200_l512.ckpt'\n",
    "# Load the model\n",
    "saver.restore(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "Once the model is trained. We are going to generate text from trained models. The idea is that we pass in a character, then the network will predict the next character. We can use the new one, to predict the next one. And we keep doing this to generate all new text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `pick_top_n` function will pick the predictions from RNN model with each class probability. \n",
    "* `np.squeeze` function used to remove one-dimensional entry from the shape of the given array. Where the preds comes as 3-dimension array.\n",
    "* `np.argsort` Soft the array by probability of each class. and pick the `top_n` charactors.\n",
    "* `np.random.choice` Create random samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    # remove 1 dimension\n",
    "    p = np.squeeze(preds)\n",
    "    # Sort by probability\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* loop through each letter to predict next one\n",
    "* load the model from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: (1, 1)\n",
      "Shape of the output tensor: (1, 1)\n",
      "ALL LSTM cells creation: <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fbc8685f198>\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i3960_l512.ckpt\n",
      "Farred at the table that had\n",
      "so much fell of. Here she could not take the clotices of the servants, and\n",
      "were all the crowd of the servant and his company of the point of the\n",
      "conviction. She was that the person had never drawing the theer\n",
      "of them. He found him to be decided abluttly at his hear, which\n",
      "had been at once thank women in the princess, and he had not spoken\n",
      "himself at the thick three hay at feeling in her arms to the solicary\n",
      "of them to be dead at the stars and was taken. He sere to say if his\n",
      "conversation. She came into the side of the bell of hand, and so she\n",
      "did not know her and show her at her and with a fact that in her seaken he\n",
      "did not spot them and to take the three, he heard her at it, and\n",
      "the sheet of the cappity of the portrait that had asked him. They still\n",
      "make him to be done. He was not at the sort of answer in which she was, that he\n",
      "were a strengt, they had been said. And at the tea on the country she\n",
      "was crimalory and tought. There was something still so that he would\n",
      "not have been standing and defects to say them. She was a calling\n",
      "sunshalistent face. And that he was not in the further of a portrait,\n",
      "and all the mistakes thistering the crowd had been drinking and dascined\n",
      "to his mother and all over. The sound of the discribbion of the charm and\n",
      "an acted and with the prince should tike the fortule--one would have been\n",
      "transing him in their father, had thar nothing of his feelings had no serve\n",
      "would be happy, all the chief common, when he was at the picture--it, which\n",
      "he was to blas in his ball for him, too, though the moment he could not be\n",
      "clutching at the chief passion. Her head should see it, and that it was\n",
      "immediately who was in his face, and how she was a state of her faint\n",
      "angry.\n",
      "\n",
      "\"I've not the same piece of me,\" the called was something.\n",
      "\n",
      "\"What wasting your subject.\"\n",
      "\n",
      "\"I answer that I shund it's a crish was any ofeen.\"\n",
      "\n",
      "\"I am going to say, the done of that, and I want nothing between her and\n",
      "has so done with that time, to do it,\" the co\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: (1, 1)\n",
      "Shape of the output tensor: (1, 1)\n",
      "ALL LSTM cells creation: <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fbc3d65b208>\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i600_l512.ckpt\n",
      "Fartre\n",
      "dor the contelssion, all atent the portings, and the his tround. And that he wan to his seaning.\n",
      "\n",
      "\"At what so what he said nathing to\n",
      "him betally, bucting and as anding her\n",
      "tore anytred hard and aspents with the sanch of\n",
      "shins the concenss to\n",
      "a pastion and the sach him\n",
      "the withous, he distion the\n",
      "poress of stard.\n",
      "\n",
      "\"I mant which and she his sereary, by sheat he called, the she there aness of himserfing tanding to the\n",
      "muther and\n",
      "shathing her. \"I dint to the more the poncertens that the\n",
      "mane and the crond and the mored. Ho saik of and his\n",
      "sine then\n",
      "well talk in the hid his, thind she had hould\n",
      "not his take her sending,\n",
      "was had to her\n",
      "herdent, her, the center had beang his her had hearse, and has stired and him sounded the\n",
      "pricester the semint of the\n",
      "prasting, she with\n",
      "which as the compors and he sand ofter an to bean ofteling at\n",
      "to his said the his and his this hissilf. And hered her thought had been at the somplote her had aspored and\n",
      "that whet he chald the had\n",
      "stired had. Anna a said\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i600_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the input tensor: (1, 1)\n",
      "Shape of the output tensor: (1, 1)\n",
      "ALL LSTM cells creation: <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fbc61707a20>\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i1200_l512.ckpt\n",
      "Fardraly. But she was a strange with his condiciat of the parsiant of her said to a cannon and\n",
      "sear of the sare and something with all, he was striggly and say to\n",
      "the sants of serent of him and he\n",
      "sow, her ale\n",
      "to be any the conversation, he shouted to\n",
      "her at the parents and at her fanch the clast, she\n",
      "were at the most as though the\n",
      "more of the come, but the man as any she went\n",
      "to think the coldections that she would seen her\n",
      "husbandd him he had not\n",
      "three wither trat and that had a conversation that\n",
      "she went out off her find the camment to seen her face all the position in\n",
      "socition in the comacion was the\n",
      "poon who conversatial of her for the same of\n",
      "the complething were all, so the sense of\n",
      "happy of the stanting of the some out of a lateres,\n",
      "what\n",
      "he heard that her would have believed,\n",
      "and stopped her all the prance.\n",
      "\n",
      "Stepan Arkadyevitch, and she saw it seem it. And she was all the can had been tone to\n",
      "see to mate it and shoute of ant seet all the statter, and and\n",
      "which would never say and a\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'checkpoints/i1200_l512.ckpt'\n",
    "samp = sample(checkpoint, 1000, lstm_size, len(vocab), prime=\"Far\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
